///|
/// Tokenizer: åœ¨è®¡ç®—æœºç§‘å­¦ä¸­ï¼Œåˆ†è¯å™¨æ˜¯ä¸€ç§ç”¨äºŽå°†æ–‡æœ¬åˆ†è§£æˆå•è¯ã€çŸ­è¯­æˆ–ç¬¦å·çš„å·¥å…·ï¼Œå¸¸ç”¨äºŽè‡ªç„¶è¯­è¨€å¤„ç†ã€‚
/// cursor: åœ¨è®¡ç®—æœºç¼–ç¨‹ä¸­ï¼Œcursoré€šå¸¸æŒ‡å±å¹•ä¸Šçš„å…‰æ ‡ï¼Œç”¨äºŽæŒ‡ç¤ºæ–‡æœ¬è¾“å…¥çš„ä½ç½®ã€‚
/// cur_token: åœ¨ç¼–ç¨‹ä¸­ï¼Œ'cur_token'é€šå¸¸æŒ‡çš„æ˜¯å½“å‰æ­£åœ¨è¢«è§£æžæˆ–å¤„ç†çš„æ ‡è®°ï¼Œå¦‚å­—ç¬¦æˆ–ç¬¦å·ã€‚
struct Tokenizer {
  mut token_start : Int
  mut cursor : Int
  src : String
  mut cur_token : Token
}

///|
/// IDENT: åœ¨ç¼–ç¨‹è¯­è¨€ä¸­,'IDENT'é€šå¸¸æŒ‡çš„æ˜¯æ ‡è¯†ç¬¦ï¼Œå³ç”¨äºŽå‘½åå˜é‡ã€å‡½æ•°ç­‰çš„åç§°
/// EOF: æ–‡ä»¶ç»“æŸæ ‡è®°
priv enum Token {
  LET
  EQUAL
  IN
  FUN
  ARROW
  IDENT(String)
  CON(String)
  LPAREN
  RPAREN
  EOF
} derive(Show, Eq)

///|
enum Expr {
  Var(String)
  Con(String)
  Lam(String, Expr)
  App(Expr, Expr)
  Let(String, Expr, Expr)
} derive(Show)

///|
/// BadTokenError: æ— æ•ˆç¬¦å·é”™è¯¯
/// InvalidCharacterError: æ— æ•ˆå­—ç¬¦é”™è¯¯
/// UnexpectedEofError: æ„å¤–EOFé”™è¯¯
suberror TokenError {
  BadTokenError(String)
  InvalidCharacterError(String)
  UnexpectedEofError
  UnexpectedTokenError(String)
} derive(Show)

///|
pub fn Tokenizer::make(src : String) -> Tokenizer {
  {token_start: 0 , cursor: 0, src, cur_token: EOF}
}

///|
pub fn parse_whole_file(self : Tokenizer) -> Expr raise TokenError {
  let expr = self.parse_expr()
  self.expect_token(EOF)
  expr
}

///|
fn parse_expr(self : Tokenizer) -> Expr raise TokenError {
  match self.peek(){
    EOF => raise UnexpectedEofError
    LET => {
      let _ = self.next()
      let id =  self.expect_ident()
      self.expect_token(EQUAL)
      let rhs = self.parse_expr()
      self.expect_token(IN)
      let body = self.parse_expr()
      Let(id, rhs, body)
    }
    FUN => {
      let _ = self.next()
      let param = self.expect_ident()
      self.expect_token(ARROW)
      let body = self.parse_expr()
      Lam(param, body)
    }
    _ => {
      let mut head = self.parse_simple_expr()
      while true {
        match self.peek() {
          LPAREN | IDENT(_) | CON(_) => {
            let arg = self.parse_simple_expr()
            head = App(head, arg)
          }
          _ => break
        }
      }
      head
    }     
  }
}

///|
/// peek: 'peek'é€šå¸¸æŒ‡åœ¨ä¸æ¶ˆè€—è¾“å…¥çš„æƒ…å†µä¸‹æŸ¥çœ‹ä¸‹ä¸€ä¸ªè¾“å…¥é¡¹ã€‚
fn peek(self : Tokenizer) -> Token raise TokenError {
  if self.cursor == 0 {
    self.cur_token = self.do_next()
  }
  self.cur_token
}

///|
/// å†™çš„å¤ªæ£’äº†ðŸ˜€
fn do_next(self : Tokenizer) -> Token raise TokenError {
  self.token_start = self.cursor
  let c = match self.next_char(){
    None =>  return EOF
    Some(c) => c
  }
  match c {
    ' ' | '\n' | '\t' => return self.do_next()
    '(' => return LPAREN
    ')' => return RPAREN
    '=' => return EQUAL
    '-' => 
      match self.next_char() {
        Some('>') => return ARROW
        _ => raise BadTokenError("bad token \"" + self.lexeme() + "\"")
      }
    _ => ()
  }
  if ('a' <= c && c <= 'z') || c == '_' {
    match self.nex_ident() {
      "let" => LET
      "in" => IN
      "fun" => FUN
      id => IDENT(id)
    }
  } else if 'A' <= c && c <= 'Z' {
    CON(self.nex_ident())
  } else {
    raise InvalidCharacterError("invalid character" + c.to_string())
  }
}

///|
fn next_char(self : Tokenizer) -> Char? {
  let c = self.peek_char()
  match c {
    Some(_) => self.cursor = self.cursor + 1
    None => ()
  }
  c
}

///|
fn peek_char(self : Tokenizer) -> Char? {
  if self.cursor >= self.src.length() {
    return None
  }
  Some(self.src.get_char(self.cursor).unwrap())
}

///|
/// lexeme: The word 'cat' consists of three lexemes: 'c', 'a', and 't'.
fn lexeme(self: Tokenizer) -> String {
  self.src.substring(start=self.token_start, end=self.cursor)
}

///|
fn nex_ident(self : Tokenizer) -> String {
  match self.peek_char() {
    None => self.lexeme()
    Some(c) => 
      if ('a' <= c && c <= 'z') ||
        ('A' <= c && c <= 'Z') ||
        ('0' <= c && c <= '9') ||
        c == '_' {
          self.cursor = self.cursor + 1
          self.nex_ident()
      } else {
        self.lexeme()
      }
  }
}

///|
fn next(self : Tokenizer) -> Token raise TokenError {
  let tok = self.peek()
  self.cur_token = self.do_next()
  tok
}

///| â€˜ \" â€™ è½¬ä¹‰çš„æ˜¯ â€˜ " â€™, â€˜\{tok}â€™ è½¬ä¹‰ tokæ’å€¼
fn expect_ident(self : Tokenizer) -> String raise TokenError {
  match self.next() {
    IDENT(id) => id
    token => {
      let tok = token.to_string() 
      raise UnexpectedTokenError(
        "unexpexted token \"\{tok}\", expecting identifier"
      )
    }
  }
}

///|
fn expect_token(self : Tokenizer, tok : Token) -> Unit raise TokenError {
  let next_tok = self.next()
  if next_tok == tok {
    
  } else {
    raise UnexpectedTokenError(
      "unexpected token \"\{next_tok}\", expecting \"\{tok}\""
    )
  }
}

///|
fn parse_simple_expr(self : Tokenizer) -> Expr raise TokenError {
  match self.next() {
    IDENT(id) => Var(id)
    CON(con) => Con(con)
    LPAREN => {
      let expr = self.parse_expr()
      self.expect_token(RPAREN)
      expr
    }
    token => {
      let tok = token.to_string()
      raise UnexpectedTokenError(
        "unexpected token \"\{tok}\", expecting simple expression"
      )
    }
  }
}